library(rpart)
library(mlbench)
library(caret)
library(party)
##Question 3##
#Ensemble Model 2: Random Forest------------------------------------------------------
library(randomForest)
#Compare CART with Bagging and RandomForests
library(ggplot2)
library(caret)
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)
##Question 6##
#Ensemble Model 4: Adaboost -----------------------------------------------
library(ada)
library(adabag)
library(tidyverse)
gs <- list(iter = c(50,100,200),
bag.frac = c(0.1,0.25,0.5)) %>%
cross_df()
ada_perf <- data.frame()
gc()
iter_cnt = 1
for(i in 1:nrow(gs)){
cat("Training adaboost: the number of population:",gs$iter[i],",ratio:",gs$bag.frac[i],"\n")
evaluation <- c()
tmp_iter <- gs[i,1]
tmp_frac <- gs[i,2]
tmp_adaboost <- boosting(GradeYN ~., data = CART_trn, boos = TRUE, mfinal = gs$iter[i], bag.frac = gs$bag.frac[i], control = rpart.control(mincriterion = 0.9, minsplit = 10))
real <- CART_val$GradeYN
pred <- predict(tmp_adaboost, CART_val[,input_idx])
evaluation <- rbind(evaluation, cbind(real,pred$class))
cfm <- matrix(0,nrow = 3, ncol = 3)
cfm[1,1] <- length(which(evaluation[,1] == 1 & evaluation[,2] == 1))
cfm[1,2] <- length(which(evaluation[,1] == 1 & evaluation[,2] == 2))
cfm[1,3] <- length(which(evaluation[,1] == 1 & evaluation[,2] == 3))
cfm[2,1] <- length(which(evaluation[,1] == 2 & evaluation[,2] == 1))
cfm[2,2] <- length(which(evaluation[,1] == 2 & evaluation[,2] == 2))
cfm[2,3] <- length(which(evaluation[,1] == 2 & evaluation[,2] == 3))
cfm[3,1] <- length(which(evaluation[,1] == 3 & evaluation[,2] == 1))
cfm[3,2] <- length(which(evaluation[,1] == 3 & evaluation[,2] == 2))
cfm[3,3] <- length(which(evaluation[,1] == 3 & evaluation[,2] == 3))
tmp_gs <- data.frame(tmp_iter, tmp_frac)
tmp_ada <- t(perf_eval_multi(cfm))
ada_perf <- rbind(ada_perf,cbind(tmp_gs,tmp_ada))
}
ptm <- proc.time()
adaboost.model <- boosting(GradeYN ~., data = CART_data, boos = TRUE, iter = 50, bag.frac = 0.5)
Boosting.Time <- proc.time() - ptm
Boosting.Time
adaboost.pred <- predict(adaboost.model,CART_tst[,input_idx])
adaboost.cfm <- table(CART_tst$GradeYN, adaboost.pred$class)
adaboost.cfm
perf_table[7,] <- perf_eval_multi(adaboost.cfm)
perf_table
##Question 7##
#Ensemble Model 5: GBM -------------------------------------
library(gbm)
library(caret)
GBM.trn <- data.frame(trn[,input_idx],GradeYN = trn[,target_idx])
GBM.val <- data.frame(val[,input_idx],GradeYN = val[,target_idx])
GBM.tst <- data.frame(tst[,input_idx],GradeYN = tst[,target_idx])
gbmGrid <-  expand.grid(n.trees = c(400,500,600,700),
shrinkage = c(0.05, 0.1, 0.15, 0.2))
ptm <- proc.time()
gbm.model <- gbm.fit(data[,input_idx],data[,target_idx], distribution = "multinomial",verbose = TRUE, n.trees = 700, shrinkage = 0.2)
gbm.Time <- proc.time() - ptm
gbm.Time
summary <- summary(gbm.model)
summary <- summary(gbm.model)
gbm.pred <- as.data.frame(predict(gbm.model, GBM.tst[,input_idx], type = "response", n.trees = best_tree))
gbm.cfm <- table(max.col(gbm.pred), GBM.tst$GradeYN)
gbm.cfm
perf_table[8,] <- perf_eval_multi(gbm.cfm)
perf_table
gbm.pred <- as.data.frame(predict(gbm.model, GBM.tst[,input_idx], type = "response", n.trees = 700))
gbm.cfm <- table(max.col(gbm.pred), GBM.tst$GradeYN)
gbm.cfm
summary
summary
##Question 3##
#Ensemble Model 2: Random Forest------------------------------------------------------
library(randomForest)
ntree <- seq(from = 30, to = 300, by = 30)
RF_Result <- matrix(0,length(ntree),3)
colnames(RF_Result) <- c("Tree","ACC","BCR")
iter_cnt = 1
for(i in 1:length(ntree)){
cat("RandomForest Training:",ntree[i],"\n")
tmp_RF <- randomForest(GradeYN ~., data = CART_trn, ntree = ntree[i], mincriterion = best_criterion, min_split = best_split, maxdepth = max_depth, importance = TRUE, do.trace = TRUE)
RF.pred <- predict(tmp_RF, newdata = CART_val, type = "class")
RF.cfm <- table(CART_val$GradeYN, RF.pred)
print(tmp_RF)
RF_Result[iter_cnt,1] = ntree[i]
RF_Result[iter_cnt,2:3] = t(perf_eval_multi(RF.cfm))
iter_cnt = iter_cnt + 1
}
ptm <- proc.time()
RF_Final <- randomForest(GradeYN ~ ., data = CART_data, ntree = 30, importance = TRUE, do.trace = TRUE)
RF.Time <- ptm - proc.time()
print(RF_Final)
plot(RF_Final)
Var.imp <- importance(RF_Final)
impor <- Var.imp[order(Var.imp[,4],decreasing = TRUE),]
summary(Var.imp)
impor
summary
summary
##Extra Question##
#Optimal Ensemble Model: randomForest------------------------------------------------------
library(randomForest)
set.seed(12345)
set.seed(12345)
trn_smote <- damage_smote[sample(nrow(damage_smote),4385),]
#Method1: Upsampling
library(caret)
Damage_up <- upSample(subset(Final_data, select = -Class),Final_data$Class)
table(Damage_up$Class)
RF.ensem <- randomForest(GradeYN ~., data = CART_trn_up, ntree = 300, mincriterion = best_criterion, min_split = best_split, maxdepth = max_depth, importance = TRUE, do.trace = TRUE)
RF.ensem <- randomForest(GradeYN ~., data = CART_trn, ntree = 30, mincriterion = 0.9, min_split = 10, maxdepth = 0, importance = TRUE, do.trace = TRUE)
Damage_up
Damage_up
Damage_up <- upSample(subset(Final_data, select = -Class),Final_data$Class)
table(Damage_up$Class)
##Extra Question##
#Optimal Ensemble Model: randomForest------------------------------------------------------
library(randomForest)
set.seed(12345)
trn_smote <- damage_smote[sample(nrow(damage_smote),4385),]
val_smote <- damage_smote[sample(nrow(damage_smote),1512),]
tst_smote <- damage_smote[sample(nrow(damage_smote),1663),]
damage_smote <- SMOTE(Class ~., data = Final_data, perc.over = 500, per.under = 50)
#Method3: SMOTE
library(DMwR)
#Method2: DownSampling
library(UBL)
#Method1: Upsampling
library(caret)
#Method3: SMOTE
library(DMwR)
install.packages(DMwR)
class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],14825)
class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],14825)
class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],14825)
#for under sampling
class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],2520)
class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],2520)
Damage <- rbind(class_1,class_2,class_3)
library(nnet)
cha_idx <- c(9:15,27)
character <- Damage[,cha_idx]
a <- names(character)
frame <- 0
for(i in 1:length(character)){
tmp <- class.ind(character[,i])
for(j in 1:ncol(tmp)){
colnames(tmp)[j] <-  paste0(a[i],"_",colnames(tmp)[j])
}
assign(paste0("dummy_",a[i]),tmp)
frame <- cbind(frame,tmp)
}
cha_input <- frame[,c(2:39)]
Damage <- read.csv("Earthquake_Damage.csv")
#Reduce Data set
library(dplyr)
#for real sampling
#class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
#class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],14825)
#class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],8725)
#for under sampling
class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],2520)
class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],2520)
Damage <- rbind(class_1,class_2,class_3)
##Data Preparation##
#Convert character variable to binary
library(nnet)
cha_idx <- c(9:15,27)
character <- Damage[,cha_idx]
a <- names(character)
frame <- 0
for(i in 1:length(character)){
tmp <- class.ind(character[,i])
for(j in 1:ncol(tmp)){
colnames(tmp)[j] <-  paste0(a[i],"_",colnames(tmp)[j])
}
assign(paste0("dummy_",a[i]),tmp)
frame <- cbind(frame,tmp)
}
cha_input <- frame[,c(2:39)]
Damage <- read.csv("Earthquake_Damage.csv")
#Reduce Data set
library(dplyr)
#for real sampling
#class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
#class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],14825)
#class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],8725)
#for under sampling
class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],2520)
class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],2520)
Damage <- rbind(class_1,class_2,class_3)
##Data Preparation##
#Convert character variable to binary
library(nnet)
cha_idx <- c(9:15,27)
character <- Damage[,cha_idx]
a <- names(character)
frame <- 0
for(i in 1:length(character)){
tmp <- class.ind(character[,i])
for(j in 1:ncol(tmp)){
colnames(tmp)[j] <-  paste0(a[i],"_",colnames(tmp)[j])
}
assign(paste0("dummy_",a[i]),tmp)
frame <- cbind(frame,tmp)
}
cha_input <- frame[,c(2:39)]
View(frame)
Damage <- read.csv("Earthquake_Damage.csv")
#Reduce Data set
library(dplyr)
#for real sampling
#class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
#class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],14825)
#class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],8725)
#for under sampling
class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],2520)
class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],2520)
Damage <- rbind(class_1,class_2,class_3)
##Data Preparation##
#Convert character variable to binary
library(nnet)
cha_idx <- c(9:15,27)
character <- Damage[,cha_idx]
a <- names(character)
frame <- 0
for(i in 1:length(character)){
tmp <- class.ind(character[,i])
for(j in 1:ncol(tmp)){
colnames(tmp)[j] <-  paste0(a[i],"_",colnames(tmp)[j])
}
assign(paste0("dummy_",a[i]),tmp)
frame <- cbind(frame,tmp)
}
cha_input <- frame[,c(2:39)]
Damage <- read.csv("Earthquake_Damage.csv")
#Reduce Data set
library(dplyr)
#for real sampling
#class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
#class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],14825)
#class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],8725)
#for under sampling
class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],2520)
class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],2520)
Damage <- rbind(class_1,class_2,class_3)
##Data Preparation##
#Convert character variable to binary
library(nnet)
cha_idx <- c(9:15,27)
character <- Damage[,cha_idx]
a <- names(character)
frame <- 0
for(i in 1:length(character)){
tmp <- class.ind(character[,i])
for(j in 1:ncol(tmp)){
colnames(tmp)[j] <-  paste0(a[i],"_",colnames(tmp)[j])
}
assign(paste0("dummy_",a[i]),tmp)
frame <- cbind(frame,tmp)
}
cha_input <- frame[,c(2:39)]
#Convert non numeric variables to binary and combine to 1 dataframe
num_idx <- c(2:8,28)
num_input <- scale(Damage[,num_idx], center = TRUE, scale = TRUE)
bin_idx <- c(16:26,29:39)
bin_input <- lapply(Damage[,bin_idx],factor)
input <- data.frame(cha_input,num_input,bin_input)
target <- as.factor(Damage[,c(40)])
Final_data <- data.frame(input, Class = target)
set.seed(12345)
trn <- Final_data[sample(nrow(Final_data),4310),]
val <- Final_data[sample(nrow(Final_data),1436),]
tst <- Final_data[sample(nrow(Final_data),1814),]
#Performance Evaluation
perf_eval_multi <- function(cm){
#Simple Accuracy
ACC <- sum(diag(cm))/sum(cm)
#Balanced Correction Rate
BCR <- 1
for(i in 1:dim(cm)[1]){
BCR = BCR * (cm[i,i]/sum(cm[i,]))
}
BCR = BCR^(1/dim(cm)[1])
return(c(ACC,BCR))
}
#Performance Evaluation
perf_eval_multi <- function(cm){
#Simple Accuracy
ACC <- sum(diag(cm))/sum(cm)
#Balanced Correction Rate
BCR <- 1
for(i in 1:dim(cm)[1]){
BCR = BCR * (cm[i,i]/sum(cm[i,]))
}
BCR = BCR^(1/dim(cm)[1])
return(c(ACC,BCR))
}
perf_table <- matrix(0,nrow = 8, ncol = 2)
colnames(perf_table) <- c("ACC","BCR")
rownames(perf_table) <- c("MLR","CART","ANN","Bagging CART","Random Forests","Bagging ANN","AdaBoost","GBM")
##Question 1##
#Single Model 1: Multinomial Logistic Regression----------------------------------
trn_data <- rbind(trn,val)
library(party)
library(tidyverse)
library(dplyr)
input_idx <- c(1:68)
target_idx <- 69
set.seed(12345)
CART_trn <- data.frame(trn[,input_idx], GradeYN = trn[,target_idx])
CART_val <- data.frame(val[,input_idx], GradeYN = val[,target_idx])
CART_tst <- data.frame(tst[,input_idx], GradeYN = tst[,target_idx])
#Artificial Neural Network
library(tidyverse)
#Train bagged model
library(ipred)
library(rpart)
library(mlbench)
library(caret)
library(party)
set.seed(12345)
CART_data <- rbind(CART_trn,CART_val)
set.seed(12345)
#Train bagged model
library(ipred)
library(rpart)
library(mlbench)
library(caret)
library(party)
nbagg <- seq(from = 30, to
nbagg <- seq(from = 30, to
nbagg <- seq(from = 30, to = 300, by = 30)
Bagging_Result <- matrix(0,length(nbagg),3)
ptm <- proc.time()
RF_Final <- randomForest(GradeYN ~ ., data = CART_data, ntree = 30, importance = TRUE, do.trace = TRUE)
RF.Time <- ptm - proc.time()
print(RF_Final)
plot(RF_Final)
Var.imp <- importance(RF_Final)
impor <- Var.imp[order(Var.imp[,4],decreasing = TRUE),]
summary(Var.imp)
barplot(Var.imp[order(Var.imp[,4],decreasing = TRUE),4])
RF_pred <- predict(RF_Final, newdata = CART_tst, type = "class")
RF_cfm <- table(CART_tst$GradeYN, RF_pred)
RF_cfm
perf_table[5,] <- perf_eval_multi(RF_cfm)
perf_table
#Extra Question
#upample
library(caret)
Damage_up <- upSample(subset(Final_data, select = -Class),Final_data$Class)
Damage <- read.csv("Earthquake_Damage.csv")
#Reduce Data set
library(dplyr)
#for real sampling
class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],14825)
class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],8725)
#for under sampling
#class_1 <- sample_n(Damage[which(Damage$damage_grade == 1),],2520)
#class_2 <- sample_n(Damage[which(Damage$damage_grade == 2),],2520)
#class_3 <- sample_n(Damage[which(Damage$damage_grade == 3),],2520)
Damage <- rbind(class_1,class_2,class_3)
##Data Preparation##
library(nnet)
cha_idx <- c(9:15,27)
character <- Damage[,cha_idx]
a <- names(character)
frame <- 0
for(i in 1:length(character)){
tmp <- class.ind(character[,i])
for(j in 1:ncol(tmp)){
colnames(tmp)[j] <-  paste0(a[i],"_",colnames(tmp)[j])
}
assign(paste0("dummy_",a[i]),tmp)
frame <- cbind(frame,tmp)
}
cha_input <- frame[,c(2:39)]
num_idx <- c(2:8,28)
num_input <- scale(Damage[,num_idx], center = TRUE, scale = TRUE)
bin_idx <- c(16:26,29:39)
bin_input <- lapply(Damage[,bin_idx],factor)
input <- data.frame(cha_input,num_input,bin_input)
target <- as.factor(Damage[,c(40)])
Final_data <- data.frame(input, Class = target)
#Split the Data
#set.seed(12345)
trn <- Final_data[sample(nrow(Final_data),7430),]
val <- Final_data[sample(nrow(Final_data),2477),]
tst <- Final_data[sample(nrow(Final_data),3128),]
#Performance Evaluation
perf_eval_multi <- function(cm){
#Simple Accuracy
ACC <- sum(diag(cm))/sum(cm)
#Balanced Correction Rate
BCR <- 1
for(i in 1:dim(cm)[1]){
BCR = BCR * (cm[i,i]/sum(cm[i,]))
}
BCR = BCR^(1/dim(cm)[1])
return(c(ACC,BCR))
}
perf_table <- matrix(0,nrow = 8, ncol = 2)
colnames(perf_table) <- c("ACC","BCR")
rownames(perf_table) <- c("MLR","CART","ANN","Bagging CART","Random Forests","Bagging ANN","AdaBoost","GBM")
#Question 1
# Multinomial Logistic Regression----------------------------------
trn_data <- rbind(trn,val)
#Classification and Regression Tree
library(party)
library(tidyverse)
library(dplyr)
input_idx <- c(1:68)
target_idx <- 69
set.seed(12345)
CART_trn <- data.frame(trn[,input_idx], GradeYN = trn[,target_idx])
CART_val <- data.frame(val[,input_idx], GradeYN = val[,target_idx])
CART_tst <- data.frame(tst[,input_idx], GradeYN = tst[,target_idx])
CART_data <- rbind(CART_trn,CART_val)
#Artificial Neural Network
library(tidyverse)
input <- trn[,input_idx]
target <- class.ind(trn[,target_idx])
val_input <- val[,input_idx]
val_target <- class.ind(val[,target_idx])
data <- rbind(trn,val)
data_input <- data[,input_idx]
data_target <- class.ind(data[,target_idx])
tst_input <- tst[,input_idx]
tst_target <- class.ind(tst[,target_idx])
library(ipred)
library(rpart)
library(mlbench)
library(caret)
library(party)
set.seed(12345)
nbagg <- seq(from = 30, to = 300, by = 30)
Bagging_Result <- matrix(0,length(nbagg),3)
iter_cnt = 1
#Question 3
#Random Forest
library(randomForest)
#Extra Question
#upample
library(caret)
Damage_up <- upSample(subset(Final_data, select = -Class),Final_data$Class)
table(Damage_up$Class)
RF.ensem <- randomForest(GradeYN ~., data = CART_trn_up, ntree = 300, mincriterion = best_criterion, min_split = best_split, maxdepth = max_depth, importance = TRUE, do.trace = TRUE)
CART_trn_up <- data.frame(trn[,input_idx], GradeYN = trn[,target_idx])
CART_val_up <- data.frame(val[,input_idx], GradeYN = val[,target_idx])
CART_tst_up <- data.frame(tst[,input_idx], GradeYN = tst[,target_idx])
RF.ensem <- randomForest(GradeYN ~., data = CART_trn_up, ntree = 300, mincriterion = 0.9, min_split = 10, maxdepth = 0, importance = TRUE, do.trace = TRUE)
RF.ensem.pred <- predict(RF.ensem, newdata = CART_tst_up, type = "class")
RF.ensem.cfm <- table(CART_tst_up$GradeYN,RF.ensem.pred)
RF.ensem.cfm
perf.table.extra[1,] <- perf_eval_multi(RF.ensem.cfm)
#Performance Evaluation
perf_eval_multi <- function(cm){
#Simple Accuracy
ACC <- sum(diag(cm))/sum(cm)
#Balanced Correction Rate
BCR <- 1
for(i in 1:dim(cm)[1]){
BCR = BCR * (cm[i,i]/sum(cm[i,]))
}
BCR = BCR^(1/dim(cm)[1])
return(c(ACC,BCR))
}
#Extra Question
perf_table.extra <- matrix(0,nrow = 1, ncol = 2)
colnames(perf_table.extra) <- c("ACC","BCR")
rownames(perf_table.extra) <- c("result")
perf.table.extra[1,] <- perf_eval_multi(RF.ensem.cfm)
perf_table.extra[1,] <- perf_eval_multi(RF.ensem.cfm)
perf_table.extra
print(RF.ensem)
plot(RF.ensem)
Var.imp.up <- importance(RF.ensem)
summary(Var.imp.up)
barplot(Var.imp.up[order(Var.imp.up[,4],decreasing = TRUE),4])
Damage_up <- upSample(subset(Final_data, select = -Class),Final_data$Class)
table(Damage_up$Class)
trn <- Final_data[sample(nrow(Final_data),7430),]
val <- Final_data[sample(nrow(Final_data),2477),]
tst <- Final_data[sample(nrow(Final_data),3128),]
CART_trn_up <- data.frame(trn[,input_idx], GradeYN = trn[,target_idx])
CART_val_up <- data.frame(val[,input_idx], GradeYN = val[,target_idx])
CART_tst_up <- data.frame(tst[,input_idx], GradeYN = tst[,target_idx])
CART_trn_up
View(CART_trn_up)
table(CART_tst_up$Class)
#upample
library(caret)
Damage_up <- upSample(subset(Final_data, select = -Class),Final_data$Class)
table(Damage_up$Class)
class_1 <- sample_n(Damage[which(Damage_up$damage_grade == 1),],14825)
